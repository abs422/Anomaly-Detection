{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "90AgTy1TAwlw"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# from sklearn.externals import joblib\n",
        "import seaborn as sns\n",
        "sns.set(color_codes=True)\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "\n",
        "from numpy.random import seed\n",
        "# from tensorflow import set_random_seed\n",
        "import tensorflow as tf\n",
        "# tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\n",
        "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "import os\n",
        "# os.chdir('C:/Lehigh/Rick- Anomaly Detection/Code')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/CNN-KF/Clean_data.csv\")\n",
        "\n",
        "df = df.rename({'InVehicle_Longitudinal_Speed':'inveh_long_spd', 'GPS_Speed': 'gps_speed', 'InVehicle_Longitudinal_Accel': 'inveh_long_acc'}, axis = 1)\n",
        "\n",
        "df = df.dropna()\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruiOz2qYK3-O",
        "outputId": "dac0bbc3-ef79-4a5a-c015-8a63db5889f7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "simulated_df = pd.read_csv(\"/content/drive/MyDrive/CNN-KF/Instant attack/25/Sensor3_values_instant_25.csv\", index_col = 'Unnamed: 0')\n",
        "simulated_gt = pd.read_csv(\"/content/drive/MyDrive/CNN-KF/Instant attack/25/Ground_truth_test_set_instant_25_sensor3.csv\")"
      ],
      "metadata": {
        "id": "ZpKFhef5LUX2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming last 5958 rows of Sensor3_values_instant_25.csv belongs to test samples\n",
        "\n",
        "simulated_df_test = simulated_df.iloc[-simulated_gt.shape[0]:]\n",
        "simulated_df_test = simulated_df_test.rename({'InVehicle_Longitudinal_Speed':'inveh_long_spd', 'GPS_Speed': 'gps_speed', 'InVehicle_Longitudinal_Accel': 'inveh_long_acc'}, axis = 1)"
      ],
      "metadata": {
        "id": "fhlOQ2WGLxkq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 24041\n",
        "test_size = len(df) - train_size\n",
        "train, test = df.iloc[0:train_size], simulated_df_test\n",
        "print(train.shape, test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdRkq08dL5WJ",
        "outputId": "053c2916-53ce-41b9-9c26-19cc3abe1499"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(24041, 3) (5958, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def time_segments_aggregate(X, interval, time_column, method=['mean']):\n",
        "    \"\"\"Aggregate values over given time span.\n",
        "    Args:\n",
        "        X (ndarray or pandas.DataFrame):\n",
        "            N-dimensional sequence of values.\n",
        "        interval (int):\n",
        "            Integer denoting time span to compute aggregation of.\n",
        "        time_column (int):\n",
        "            Column of X that contains time values.\n",
        "        method (str or list):\n",
        "            Optional. String describing aggregation method or list of strings describing multiple\n",
        "            aggregation methods. If not given, `mean` is used.\n",
        "    Returns:\n",
        "        ndarray, ndarray:\n",
        "            * Sequence of aggregated values, one column for each aggregation method.\n",
        "            * Sequence of index values (first index of each aggregated segment).\n",
        "    \"\"\"\n",
        "    if isinstance(X, np.ndarray):\n",
        "        X = pd.DataFrame(X)\n",
        "\n",
        "    X = X.sort_values(time_column).set_index(time_column)\n",
        "\n",
        "    if isinstance(method, str):\n",
        "        method = [method]\n",
        "\n",
        "    start_ts = X.index.values[0]\n",
        "    max_ts = X.index.values[-1]\n",
        "\n",
        "    values = list()\n",
        "    index = list()\n",
        "    while start_ts <= max_ts:\n",
        "        end_ts = start_ts + interval\n",
        "        subset = X.loc[start_ts:end_ts - 1]\n",
        "        aggregated = [\n",
        "            getattr(subset, agg)(skipna=True).values\n",
        "            for agg in method\n",
        "        ]\n",
        "        values.append(np.concatenate(aggregated))\n",
        "        index.append(start_ts)\n",
        "        start_ts = end_ts\n",
        "\n",
        "    return np.asarray(values), np.asarray(index)"
      ],
      "metadata": {
        "id": "9KVsF-w6O9l3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, index = time_segments_aggregate(df, interval=5, time_column='Epoch')"
      ],
      "metadata": {
        "id": "dMXfTjKtO_cC"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaler = scaler.fit(train)\n",
        "X_train = scaler.transform(train)\n",
        "# X_test = scaler.transform(test)"
      ],
      "metadata": {
        "id": "t_lIonk3L8Q9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rolling_window_sequences(X, index, window_size, target_size, step_size, target_column,\n",
        "                             drop=None, drop_windows=False):\n",
        "    \"\"\"Create rolling window sequences out of time series data.\n",
        "    The function creates an array of input sequences and an array of target sequences by rolling\n",
        "    over the input sequence with a specified window.\n",
        "    Optionally, certain values can be dropped from the sequences.\n",
        "    Args:\n",
        "        X (ndarray):\n",
        "            N-dimensional sequence to iterate over.\n",
        "        index (ndarray):\n",
        "            Array containing the index values of X.\n",
        "        window_size (int):\n",
        "            Length of the input sequences.\n",
        "        target_size (int):\n",
        "            Length of the target sequences.\n",
        "        step_size (int):\n",
        "            Indicating the number of steps to move the window forward each round.\n",
        "        target_column (int):\n",
        "            Indicating which column of X is the target.\n",
        "        drop (ndarray or None or str or float or bool):\n",
        "            Optional. Array of boolean values indicating which values of X are invalid, or value\n",
        "            indicating which value should be dropped. If not given, `None` is used.\n",
        "        drop_windows (bool):\n",
        "            Optional. Indicates whether the dropping functionality should be enabled. If not\n",
        "            given, `False` is used.\n",
        "    Returns:\n",
        "        ndarray, ndarray, ndarray, ndarray:\n",
        "            * input sequences.\n",
        "            * target sequences.\n",
        "            * first index value of each input sequence.\n",
        "            * first index value of each target sequence.\n",
        "    \"\"\"\n",
        "    out_X = list()\n",
        "    out_y = list()\n",
        "    X_index = list()\n",
        "    y_index = list()\n",
        "    target = X[:, target_column]\n",
        "\n",
        "    if drop_windows:\n",
        "        if hasattr(drop, '__len__') and (not isinstance(drop, str)):\n",
        "            if len(drop) != len(X):\n",
        "                raise Exception('Arrays `drop` and `X` must be of the same length.')\n",
        "        else:\n",
        "            if isinstance(drop, float) and np.isnan(drop):\n",
        "                drop = np.isnan(X)\n",
        "            else:\n",
        "                drop = X == drop\n",
        "\n",
        "    start = 0\n",
        "    max_start = len(X) - window_size - target_size + 1\n",
        "    while start < max_start:\n",
        "        end = start + window_size\n",
        "\n",
        "        if drop_windows:\n",
        "            drop_window = drop[start:end + target_size]\n",
        "            to_drop = np.where(drop_window)[0]\n",
        "            if to_drop.size:\n",
        "                start += to_drop[-1] + 1\n",
        "                continue\n",
        "\n",
        "        out_X.append(X[start:end])\n",
        "        out_y.append(target[end:end + target_size])\n",
        "        X_index.append(index[start])\n",
        "        y_index.append(index[end])\n",
        "        start = start + step_size\n",
        "\n",
        "    return np.asarray(out_X), np.asarray(out_y), np.asarray(X_index), np.asarray(y_index)"
      ],
      "metadata": {
        "id": "Ecf5iXPPNxos"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y, X_index, y_index = rolling_window_sequences(X_train, index, \n",
        "                                                  window_size=100, \n",
        "                                                  target_size=1, \n",
        "                                                  step_size=1,\n",
        "                                                  target_column=0)"
      ],
      "metadata": {
        "id": "nLMC6ScTN32k"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X154Tu2gRBJp",
        "outputId": "28087d81-ba6e-4dc4-be53-c99b187ae532"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5900, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check gpu envrionmental \n",
        "import tensorflow as tf\n",
        "import logging\n",
        "import math\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU') \n",
        "if gpus: \n",
        "    try: \n",
        "        for gpu in gpus: \n",
        "            tf.config.experimental.set_memory_growth(gpu, True) \n",
        "    except RuntimeError as e: \n",
        "        print(e)\n",
        "print (gpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3oJRNmMMNN7",
        "outputId": "81a3d187-1bd8-4b2b-ad43-e2403fc95c4c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LOGGER = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "FiOWYMDhMQo9"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install similaritymeasures"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnSqDIaeMZeY",
        "outputId": "571e1a13-14f4-4303-f80e-36fac25cd9d0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: similaritymeasures in /usr/local/lib/python3.8/dist-packages (0.7.0)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.8/dist-packages (from similaritymeasures) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from similaritymeasures) (1.22.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "#import tensorflow as tf\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import similaritymeasures as sm\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, Flatten, Dense, Reshape, UpSampling1D, TimeDistributed\n",
        "from tensorflow.keras.layers import Activation, Conv1D, LeakyReLU, Dropout, Add, Layer\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM as CUDNNLSTM\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from functools import partial\n",
        "from scipy import integrate, stats"
      ],
      "metadata": {
        "id": "C_Ckp9QoMVmi"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomWeightedAverage(Layer):\n",
        "    def _merge_function(self, inputs):\n",
        "        alpha = K.random_uniform((64, 1, 1))\n",
        "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
      ],
      "metadata": {
        "id": "8qeREtY5MhFo"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_encoder_layer(input_shape, encoder_reshape_shape):    \n",
        "    \n",
        "    input_layer = layers.Input(shape=input_shape)\n",
        "    \n",
        "    x = layers.Bidirectional(CUDNNLSTM(units=100, return_sequences=True))(input_layer)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(20)(x)\n",
        "    x = layers.Reshape(target_shape=encoder_reshape_shape)(x)\n",
        "    model = keras.models.Model(input_layer, x, name='encoder')\n",
        "    \n",
        "    return model\n",
        "\n",
        "def build_generator_layer(input_shape, generator_reshape_shape):\n",
        "    \n",
        "    input_layer = layers.Input(shape=input_shape)\n",
        "    \n",
        "    x = layers.Flatten()(input_layer)\n",
        "    x = layers.Dense(generator_reshape_shape[0])(x)\n",
        "    x = layers.Reshape(target_shape=generator_reshape_shape)(x)\n",
        "    x = layers.Bidirectional(CUDNNLSTM(units=64, return_sequences=True), merge_mode='concat')(x)\n",
        "    x = layers.UpSampling1D(size=2)(x)\n",
        "    x = layers.Bidirectional(CUDNNLSTM(units=64, return_sequences=True), merge_mode='concat')(x)\n",
        "    x = layers.TimeDistributed(layers.Dense(1))(x)\n",
        "    x = layers.Activation(activation='tanh')(x)\n",
        "    model = keras.models.Model(input_layer, x, name='generator')\n",
        "    \n",
        "    return model\n",
        "    \n",
        "\n",
        "def build_critic_x_layer(input_shape):\n",
        "    \n",
        "    input_layer = layers.Input(shape=input_shape)\n",
        "    \n",
        "    x = layers.Conv1D(filters=64, kernel_size=5)(input_layer)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = layers.Dropout(rate=0.25)(x)\n",
        "    x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = layers.Dropout(rate=0.25)(x)\n",
        "    x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = layers.Dropout(rate=0.25)(x)\n",
        "    x = layers.Conv1D(filters=64, kernel_size=5)(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = layers.Dropout(rate=0.25)(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(units=1)(x)\n",
        "    model = keras.models.Model(input_layer, x, name='critic_x')\n",
        "    \n",
        "    return model \n",
        "\n",
        "\n",
        "def build_critic_z_layer(input_shape):\n",
        "    \n",
        "    input_layer = layers.Input(shape=input_shape)\n",
        "    \n",
        "    x = layers.Flatten()(input_layer)\n",
        "    x = layers.Dense(units=100)(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = layers.Dropout(rate=0.2)(x)    \n",
        "    x = layers.Dense(units=100)(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "    x = layers.Dropout(rate=0.2)(x)  \n",
        "    x = layers.Dense(units=1)(x)\n",
        "    model = keras.models.Model(input_layer, x, name='critic_z')\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "NZkN7GhfMogo"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def wasserstein_loss(y_true, y_pred):\n",
        "#    return tf.reduce_mean(y_true * y_pred)\n",
        "    return K.mean(y_true * y_pred)\n"
      ],
      "metadata": {
        "id": "H-bWT-qSMvEb"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 200"
      ],
      "metadata": {
        "id": "RydkCH_tR4xC"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer Parameters\n",
        "encoder_input_shape = (window_size, 1)\n",
        "generator_input_shape = (20, 1)\n",
        "\n",
        "critic_x_input_shape = (window_size, 1)\n",
        "critic_z_input_shape = (20,1)\n",
        "\n",
        "encoder_reshape_shape = (20, 1)\n",
        "generator_reshape_shape = (window_size//2, 1) # window_size//3 <- 3 is Upsampling size\n",
        "\n",
        "learning_rate = 0.0005\n",
        "\n",
        "# Build Model\n",
        "encoder = build_encoder_layer(input_shape=encoder_input_shape,\n",
        "                              encoder_reshape_shape=encoder_reshape_shape)\n",
        "\n",
        "generator = build_generator_layer(input_shape=generator_input_shape,\n",
        "                                  generator_reshape_shape=generator_reshape_shape)\n",
        "\n",
        "critic_x = build_critic_x_layer(input_shape=critic_x_input_shape)\n",
        "critic_z = build_critic_z_layer(input_shape=critic_z_input_shape)\n",
        "\n",
        "encoder_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "generator_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "critic_x_optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "critic_z_optimizer = tf.keras.optimizers.Adam(learning_rate)"
      ],
      "metadata": {
        "id": "fT6euB5lRc0O"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LhdFk_TTkd5",
        "outputId": "e1366abd-8bbc-44cf-ea46-d10d0eac8206"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"encoder\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 200, 1)]          0         \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirectio  (None, 200, 200)         82400     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 40000)             0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 20)                800020    \n",
            "                                                                 \n",
            " reshape_4 (Reshape)         (None, 20, 1)             0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 882,420\n",
            "Trainable params: 882,420\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator.summary() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEx2Ro3mToUz",
        "outputId": "da04da2f-577f-4fc6-e7e0-9979bb59c4e7"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, 20, 1)]           0         \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 20)                0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 100)               2100      \n",
            "                                                                 \n",
            " reshape_5 (Reshape)         (None, 100, 1)            0         \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirectio  (None, 100, 128)         34304     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " up_sampling1d_2 (UpSampling  (None, 200, 128)         0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " bidirectional_8 (Bidirectio  (None, 200, 128)         99328     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " time_distributed_2 (TimeDis  (None, 200, 1)           129       \n",
            " tributed)                                                       \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 200, 1)            0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 135,861\n",
            "Trainable params: 135,861\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "critic_x.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dHpBB7JTuw9",
        "outputId": "fbe0d47e-d7c6-4497-c630-e5e1ac9129e0"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"critic_x\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_10 (InputLayer)       [(None, 200, 1)]          0         \n",
            "                                                                 \n",
            " conv1d_6 (Conv1D)           (None, 196, 64)           384       \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 196, 64)           0         \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 196, 64)           0         \n",
            "                                                                 \n",
            " conv1d_7 (Conv1D)           (None, 192, 64)           20544     \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 192, 64)           0         \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 192, 64)           0         \n",
            "                                                                 \n",
            " conv1d_8 (Conv1D)           (None, 188, 64)           20544     \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 188, 64)           0         \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 188, 64)           0         \n",
            "                                                                 \n",
            " conv1d_9 (Conv1D)           (None, 184, 64)           20544     \n",
            "                                                                 \n",
            " leaky_re_lu_10 (LeakyReLU)  (None, 184, 64)           0         \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 184, 64)           0         \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 11776)             0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 1)                 11777     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 73,793\n",
            "Trainable params: 73,793\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "critic_z.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpBZGIRhT2iB",
        "outputId": "f86e5b2b-091d-41f5-f4be-3691a22f541e"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"critic_z\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 20, 1)]           0         \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 20)                0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 100)               2100      \n",
            "                                                                 \n",
            " leaky_re_lu_11 (LeakyReLU)  (None, 100)               0         \n",
            "                                                                 \n",
            " dropout_11 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 100)               10100     \n",
            "                                                                 \n",
            " leaky_re_lu_12 (LeakyReLU)  (None, 100)               0         \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12,301\n",
            "Trainable params: 12,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 20\n",
        "shape = (window_size, 1)"
      ],
      "metadata": {
        "id": "Ei-70qOXT8Lg"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def critic_x_train_on_batch(x, z):\n",
        "    # Loss 크게 이상 없음 \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "        valid_x = critic_x(x)\n",
        "        x_ = generator(z)\n",
        "        fake_x = critic_x(x_)\n",
        "        \n",
        "        # Interpolated \n",
        "        alpha = tf.random.uniform([batch_size, 1, 1], 0.0, 1.0)\n",
        "        interpolated = alpha * x + (1 - alpha) * x_ \n",
        "        \n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            pred = critic_x(interpolated)\n",
        "        \n",
        "        grads = gp_tape.gradient(pred, interpolated)\n",
        "        grad_norm = tf.norm(tf.reshape(grads, (batch_size, -1)), axis=1)\n",
        "        gp_loss = 10.0*tf.reduce_mean(tf.square(grad_norm - 1.))\n",
        "#         grads = tf.square(grads)\n",
        "#         ddx = tf.sqrt(tf.reduce_sum(grads, axis=np.arange(1, len(grads.shape))))\n",
        "#        gp_loss = tf.reduce_mean((1.0 - ddx) ** 2)\n",
        "                \n",
        "        loss1 = wasserstein_loss(-tf.ones_like(valid_x), valid_x)\n",
        "        loss2 = wasserstein_loss(tf.ones_like(fake_x), fake_x)\n",
        "        #loss = tf.add_n([loss1, loss2, gp_loss*10.0])        \n",
        "        loss = loss1 + loss2 + gp_loss\n",
        "#        loss = tf.reduce_mean(loss)\n",
        "                        \n",
        "    gradients = tape.gradient(loss, critic_x.trainable_weights)\n",
        "    critic_x_optimizer.apply_gradients(zip(gradients, critic_x.trainable_weights))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "cCnwZzETT9mD"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def critic_z_train_on_batch(x, z):\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        z_ = encoder(x)   \n",
        "        valid_z = critic_z(z)             \n",
        "        fake_z = critic_z(z_) # <- critic_z 의 결과가 매우 않음 \n",
        "        \n",
        "        # Interpolated \n",
        "        alpha = tf.random.uniform([batch_size, 1, 1], 0.0, 1.0)\n",
        "        interpolated = alpha * z + (1 - alpha) * z_ \n",
        "                \n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            pred = critic_z(interpolated, training=True)\n",
        "            \n",
        "        grads = gp_tape.gradient(pred, interpolated)\n",
        "        grad_norm = tf.norm(tf.reshape(grads, (batch_size, -1)), axis=1)\n",
        "        gp_loss = 10.0*tf.reduce_mean(tf.square(grad_norm - 1.))\n",
        "\n",
        "#         grads = tf.square(grads)\n",
        "#         ddx = tf.sqrt(tf.reduce_sum(grads, axis=np.arange(1, len(grads.shape))))\n",
        "#         gp_loss = tf.reduce_mean((1.0 - ddx) ** 2)\n",
        "        \n",
        "        loss1 = wasserstein_loss(-tf.ones_like(valid_z), valid_z)\n",
        "        loss2 = wasserstein_loss(tf.ones_like(fake_z), fake_z) # <- 이게 미친듯이 뜀. \n",
        "        loss = loss1 + loss2 + gp_loss\n",
        "#        loss = tf.reduce_mean(loss)\n",
        "        \n",
        "    gradients = tape.gradient(loss, critic_z.trainable_weights)\n",
        "    critic_z_optimizer.apply_gradients(zip(gradients, critic_z.trainable_weights))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "1LC88GzvUFbf"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def enc_gen_train_on_batch(x, z):\n",
        "    with tf.GradientTape() as enc_tape:\n",
        "        \n",
        "        z_gen_ = encoder(x, training=True)\n",
        "        x_gen_ = generator(z, training=False)        \n",
        "        x_gen_rec = generator(z_gen_, training=False)\n",
        "        \n",
        "        fake_gen_x = critic_x(x_gen_, training=False)\n",
        "        fake_gen_z = critic_z(z_gen_, training=False)\n",
        "        \n",
        "        loss1 = wasserstein_loss(fake_gen_x, -tf.ones_like(fake_gen_x))\n",
        "        loss2 = wasserstein_loss(fake_gen_z, -tf.ones_like(fake_gen_z))\n",
        "        loss3 = 10.0*tf.reduce_mean(tf.keras.losses.MSE(x, x_gen_rec))\n",
        "\n",
        "        enc_loss = loss1 + loss2 + loss3\n",
        "        \n",
        "    gradients_encoder = enc_tape.gradient(enc_loss, encoder.trainable_weights)\n",
        "    encoder_optimizer.apply_gradients(zip(gradients_encoder, encoder.trainable_weights))\n",
        "\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "        \n",
        "        z_gen_ = encoder(x, training=False)\n",
        "        x_gen_ = generator(z, training=True)        \n",
        "        x_gen_rec = generator(z_gen_, training=True)\n",
        "        \n",
        "        fake_gen_x = critic_x(x_gen_, training=False)\n",
        "        fake_gen_z = critic_z(z_gen_, training=False)\n",
        "        \n",
        "        loss1 = wasserstein_loss(fake_gen_x, -tf.ones_like(fake_gen_x))\n",
        "        loss2 = wasserstein_loss(fake_gen_z, -tf.ones_like(fake_gen_z))\n",
        "        loss3 = 10.0*tf.reduce_mean(tf.keras.losses.MSE(x, x_gen_rec))\n",
        "\n",
        "        gen_loss = loss1 + loss2 + loss3\n",
        "        \n",
        "    gradients_generator = gen_tape.gradient(gen_loss, generator.trainable_weights)    \n",
        "    generator_optimizer.apply_gradients(zip(gradients_generator, generator.trainable_weights))    \n",
        "    return enc_loss, gen_loss"
      ],
      "metadata": {
        "id": "lV1U1TYrUJ-r"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train parameters\n",
        "batch_size = 32\n",
        "n_critics = 5\n",
        "epochs = 1000"
      ],
      "metadata": {
        "id": "eZtuYHLvUObK"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5_BD_0MUSkA",
        "outputId": "3e566684-c6e2-4154-b53d-4bdb5d581798"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5900, 100, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental_run_functions_eagerly(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGMb1a7oUVx-",
        "outputId": "8c26fa0a-635a-44b3-f18d-6d8162321c8f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From <ipython-input-67-bdb3352f611a>:1: experimental_run_functions_eagerly (from tensorflow.python.eager.polymorphic_function.quarantine) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Reshape\n",
        "X_copy = X.reshape((-1, shape[0], 1))\n",
        "# X_ = np.copy(X)\n",
        "X_copy.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqiGZWniUm3N",
        "outputId": "4325981e-4a89-4617-be70-830e1801b572"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8850, 200, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ]
}